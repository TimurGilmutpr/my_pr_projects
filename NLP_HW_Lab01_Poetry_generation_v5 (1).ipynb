{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjMV9XG8AMb2"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "42XZ_sRWAMb3"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "import string\n",
        "import os\n",
        "from random import sample\n",
        "\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lMwUqvuMAMb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda device is available\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"{} device is available\".format(device))\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPenWOy01Ooa",
        "outputId": "a92e8e33-e009-4bd4-ac12-3b1b5e1cd3f2"
      },
      "source": [
        "#### 1. Загрузка данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oB-bsEx0AMb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-05-25 14:41:01--  https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/onegin.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 262521 (256K) [text/plain]\n",
            "Saving to: ‘onegin.txt.9’\n",
            "\n",
            "onegin.txt.9        100%[===================>] 256.37K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2025-05-25 14:41:01 (2.92 MB/s) - ‘onegin.txt.9’ saved [262521/262521]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "!wget https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/onegin.txt\n",
        "\n",
        "with open('onegin.txt', 'r') as iofile:\n",
        "    text = iofile.readlines()\n",
        "\n",
        "text = \"\".join([x.replace('\\t\\t', '').lower() for x in text])\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQYpmGfR_gJ8"
      },
      "source": [
        "#### 2. Построение словаря и предобработка текста\n",
        "В данном задании требуется построить языковую модель на уровне символов. Приведем весь текст к нижнему регистру и построим словарь из всех символов в доступном корпусе текстов. Также добавим токен `<sos>`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "leCW8CvFAMb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seems fine!\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "tokens = sorted(set(text.lower())) + [\"<sos>\"]\n",
        "num_tokens = len(tokens)\n",
        "\n",
        "assert num_tokens == 84, \"Check the tokenization process\"\n",
        "\n",
        "token_to_idx = {x: idx for idx, x in enumerate(tokens)}\n",
        "idx_to_token = {idx: x for idx, x in enumerate(tokens)}\n",
        "\n",
        "assert len(tokens) == len(token_to_idx), \"Mapping should be unique\"\n",
        "\n",
        "print(\"Seems fine!\")\n",
        "\n",
        "\n",
        "text_encoded = [token_to_idx[x] for x in text]\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1sRbflMAMb5"
      },
      "source": [
        "__Ваша задача__: обучить классическую рекуррентную нейронную сеть (Vanilla RNN) предсказывать следующий символ на полученном корпусе текстов и сгенерировать последовательность длины 100 для фиксированной начальной фразы.\n",
        "\n",
        "Вы можете воспользоваться кодом с занятие №6 или же обратиться к следующим ссылкам:\n",
        "* Замечательная статья за авторством Andrej Karpathy об использовании RNN: [link](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "* Пример char-rnn от Andrej Karpathy: [github repo](https://github.com/karpathy/char-rnn)\n",
        "* Замечательный пример генерации поэзии Шекспира: [github repo](https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb)\n",
        "\n",
        "Данное задание является достаточно творческим. Не страшно, если поначалу оно вызывает затруднения. Последняя ссылка в списке выше может быть особенно полезна в данном случае.\n",
        "\n",
        "Далее для вашего удобства реализована функция, которая генерирует случайный батч размера `batch_size` из строк длиной `seq_length`. Вы можете использовать его при обучении модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "irK1eo-sAMb5"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "batch_size = 256\n",
        "seq_length = 100\n",
        "start_column = np.zeros((batch_size, 1), dtype=int) + token_to_idx[\"<sos>\"]\n",
        "\n",
        "\n",
        "def generate_chunk():\n",
        "    global text_encoded, start_column, batch_size, seq_length\n",
        "\n",
        "    start_index = np.random.randint(0, len(text_encoded) - batch_size * seq_length - 1)\n",
        "    data = np.array(\n",
        "        text_encoded[start_index : start_index + batch_size * seq_length]\n",
        "    ).reshape((batch_size, -1))\n",
        "    yield np.hstack((start_column, data))\n",
        "\n",
        "\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scEafweGAMb6"
      },
      "source": [
        "Пример батча:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yGYzxEfOAMb6",
        "outputId": "5cb1cc28-c966-4f12-d7f3-acc36c106120"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[83, 50, 50, ..., 59, 54, 13],\n",
              "       [83,  0, 47, ..., 53, 63, 62],\n",
              "       [83, 76,  1, ...,  0, 53,  1],\n",
              "       ...,\n",
              "       [83,  1, 57, ..., 56, 73, 48],\n",
              "       [83, 59, 54, ..., 49, 50,  1],\n",
              "       [83, 51,  1, ..., 14, 44,  0]])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(generate_chunk())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGPRS5wwAMb6"
      },
      "source": [
        "Далее вам предстоит написать код для обучения модели и генерации текста."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ImprovedCharRNN(nn.Module):\n",
        "    def __init__(self, num_tokens, emb_size=256, hidden_size=512, n_layers=3, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(num_tokens, emb_size)\n",
        "        self.lstm = nn.LSTM(emb_size, hidden_size, n_layers, \n",
        "                           dropout=dropout if n_layers > 1 else 0, \n",
        "                           batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_tokens)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, h_prev=None):\n",
        "        x = self.emb(x)\n",
        "        out, h = self.lstm(x, h_prev)\n",
        "        out = self.dropout(out)\n",
        "        logits = self.fc(out)\n",
        "        return logits, h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = ImprovedCharRNN(num_tokens).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 3.1491, LR: 5.00e-04\n",
            "Epoch 2, Loss: 2.5758, LR: 5.00e-04\n",
            "Epoch 3, Loss: 2.3736, LR: 5.00e-04\n",
            "Epoch 4, Loss: 2.2311, LR: 5.00e-04\n",
            "Epoch 5, Loss: 2.1175, LR: 5.00e-04\n",
            "Epoch 6, Loss: 2.0091, LR: 5.00e-04\n",
            "Epoch 7, Loss: 1.9122, LR: 5.00e-04\n",
            "Epoch 8, Loss: 1.8140, LR: 5.00e-04\n",
            "Epoch 9, Loss: 1.7349, LR: 5.00e-04\n",
            "Epoch 10, Loss: 1.6427, LR: 5.00e-04\n",
            "Epoch 11, Loss: 1.5617, LR: 5.00e-04\n",
            "Epoch 12, Loss: 1.4908, LR: 5.00e-04\n",
            "Epoch 13, Loss: 1.3921, LR: 5.00e-04\n",
            "Epoch 14, Loss: 1.3160, LR: 5.00e-04\n",
            "Epoch 15, Loss: 1.2561, LR: 5.00e-04\n",
            "Epoch 16, Loss: 1.1734, LR: 5.00e-04\n",
            "Epoch 17, Loss: 1.0918, LR: 5.00e-04\n",
            "Epoch 18, Loss: 1.0436, LR: 5.00e-04\n",
            "Epoch 19, Loss: 0.9699, LR: 5.00e-04\n",
            "Epoch 20, Loss: 0.9039, LR: 5.00e-04\n",
            "Epoch 21, Loss: 0.8560, LR: 5.00e-04\n",
            "Epoch 22, Loss: 0.8012, LR: 5.00e-04\n",
            "Epoch 23, Loss: 0.7682, LR: 5.00e-04\n",
            "Epoch 24, Loss: 0.7019, LR: 5.00e-04\n",
            "Epoch 25, Loss: 0.6329, LR: 5.00e-04\n",
            "Epoch 26, Loss: 0.5962, LR: 5.00e-04\n",
            "Epoch 27, Loss: 0.5626, LR: 5.00e-04\n",
            "Epoch 28, Loss: 0.5746, LR: 5.00e-04\n",
            "Epoch 29, Loss: 0.5276, LR: 5.00e-04\n",
            "Epoch 30, Loss: 0.4841, LR: 5.00e-04\n",
            "Epoch 31, Loss: 0.4701, LR: 5.00e-04\n",
            "Epoch 32, Loss: 0.4727, LR: 5.00e-04\n",
            "Epoch 33, Loss: 0.4340, LR: 5.00e-04\n",
            "Epoch 34, Loss: 0.4281, LR: 5.00e-04\n",
            "Epoch 35, Loss: 0.4203, LR: 5.00e-04\n",
            "Epoch 36, Loss: 0.4207, LR: 5.00e-04\n",
            "Epoch 37, Loss: 0.4115, LR: 5.00e-04\n",
            "Epoch 38, Loss: 0.3921, LR: 5.00e-04\n",
            "Epoch 39, Loss: 0.3787, LR: 5.00e-04\n",
            "Epoch 40, Loss: 0.3595, LR: 5.00e-04\n",
            "Epoch 41, Loss: 0.3689, LR: 5.00e-04\n",
            "Epoch 42, Loss: 0.3534, LR: 5.00e-04\n",
            "Epoch 43, Loss: 0.3652, LR: 5.00e-04\n",
            "Epoch 44, Loss: 0.3505, LR: 5.00e-04\n",
            "Epoch 45, Loss: 0.3506, LR: 5.00e-04\n",
            "Epoch 46, Loss: 0.3429, LR: 5.00e-04\n",
            "Epoch 47, Loss: 0.3421, LR: 5.00e-04\n",
            "Epoch 48, Loss: 0.3289, LR: 5.00e-04\n",
            "Epoch 49, Loss: 0.3368, LR: 5.00e-04\n",
            "Epoch 50, Loss: 0.3303, LR: 5.00e-04\n",
            "Epoch 51, Loss: 0.3276, LR: 5.00e-04\n",
            "Epoch 52, Loss: 0.3248, LR: 5.00e-04\n",
            "Epoch 53, Loss: 0.3261, LR: 5.00e-04\n",
            "Epoch 54, Loss: 0.3154, LR: 5.00e-04\n",
            "Epoch 55, Loss: 0.3161, LR: 5.00e-04\n",
            "Epoch 56, Loss: 0.3103, LR: 5.00e-04\n",
            "Epoch 57, Loss: 0.3119, LR: 5.00e-04\n",
            "Epoch 58, Loss: 0.2999, LR: 5.00e-04\n",
            "Epoch 59, Loss: 0.2992, LR: 5.00e-04\n",
            "Epoch 60, Loss: 0.3007, LR: 5.00e-04\n",
            "Epoch 61, Loss: 0.2973, LR: 5.00e-04\n",
            "Epoch 62, Loss: 0.2914, LR: 5.00e-04\n",
            "Epoch 63, Loss: 0.2876, LR: 5.00e-04\n",
            "Epoch 64, Loss: 0.3017, LR: 5.00e-04\n",
            "Epoch 65, Loss: 0.2900, LR: 5.00e-04\n",
            "Epoch 66, Loss: 0.2924, LR: 5.00e-04\n",
            "Epoch 67, Loss: 0.2876, LR: 5.00e-04\n",
            "Epoch 68, Loss: 0.2870, LR: 5.00e-04\n",
            "Epoch 69, Loss: 0.2844, LR: 5.00e-04\n",
            "Epoch 70, Loss: 0.2850, LR: 5.00e-04\n",
            "Epoch 71, Loss: 0.2864, LR: 5.00e-04\n",
            "Epoch 72, Loss: 0.2841, LR: 5.00e-04\n",
            "Epoch 73, Loss: 0.2791, LR: 5.00e-04\n",
            "Epoch 74, Loss: 0.2754, LR: 5.00e-04\n",
            "Epoch 75, Loss: 0.2786, LR: 5.00e-04\n",
            "Epoch 76, Loss: 0.2703, LR: 5.00e-04\n",
            "Epoch 77, Loss: 0.2745, LR: 5.00e-04\n",
            "Epoch 78, Loss: 0.2707, LR: 5.00e-04\n",
            "Epoch 79, Loss: 0.2716, LR: 5.00e-04\n",
            "Epoch 80, Loss: 0.2743, LR: 5.00e-04\n",
            "Epoch 81, Loss: 0.2649, LR: 5.00e-04\n",
            "Epoch 82, Loss: 0.2721, LR: 5.00e-04\n",
            "Epoch 83, Loss: 0.2638, LR: 5.00e-04\n",
            "Epoch 84, Loss: 0.2653, LR: 5.00e-04\n",
            "Epoch 85, Loss: 0.2655, LR: 5.00e-04\n",
            "Epoch 86, Loss: 0.2621, LR: 5.00e-04\n",
            "Epoch 87, Loss: 0.2631, LR: 5.00e-04\n",
            "Epoch 88, Loss: 0.2597, LR: 5.00e-04\n",
            "Epoch 89, Loss: 0.2559, LR: 5.00e-04\n",
            "Epoch 90, Loss: 0.2587, LR: 5.00e-04\n",
            "Epoch 91, Loss: 0.2629, LR: 5.00e-04\n",
            "Epoch 92, Loss: 0.2595, LR: 5.00e-04\n",
            "Epoch 93, Loss: 0.2632, LR: 5.00e-04\n",
            "Epoch 94, Loss: 0.2541, LR: 5.00e-04\n",
            "Epoch 95, Loss: 0.2565, LR: 5.00e-04\n",
            "Epoch 96, Loss: 0.2573, LR: 5.00e-04\n",
            "Epoch 97, Loss: 0.2503, LR: 5.00e-04\n",
            "Epoch 98, Loss: 0.2532, LR: 5.00e-04\n",
            "Epoch 99, Loss: 0.2515, LR: 5.00e-04\n",
            "Epoch 100, Loss: 0.2485, LR: 5.00e-04\n",
            "Epoch 101, Loss: 0.2538, LR: 5.00e-04\n",
            "Epoch 102, Loss: 0.2518, LR: 5.00e-04\n",
            "Epoch 103, Loss: 0.2532, LR: 5.00e-04\n",
            "Epoch 104, Loss: 0.2473, LR: 5.00e-04\n",
            "Epoch 105, Loss: 0.2515, LR: 5.00e-04\n",
            "Epoch 106, Loss: 0.2507, LR: 5.00e-04\n",
            "Epoch 107, Loss: 0.2490, LR: 5.00e-04\n",
            "Epoch 108, Loss: 0.2500, LR: 5.00e-04\n",
            "Epoch 109, Loss: 0.2432, LR: 5.00e-04\n",
            "Epoch 110, Loss: 0.2533, LR: 5.00e-04\n",
            "Epoch 111, Loss: 0.2492, LR: 5.00e-04\n",
            "Epoch 112, Loss: 0.2478, LR: 5.00e-04\n",
            "Epoch 113, Loss: 0.2415, LR: 5.00e-04\n",
            "Epoch 114, Loss: 0.2500, LR: 5.00e-04\n",
            "Epoch 115, Loss: 0.2399, LR: 5.00e-04\n",
            "Epoch 116, Loss: 0.2401, LR: 5.00e-04\n",
            "Epoch 117, Loss: 0.2456, LR: 5.00e-04\n",
            "Epoch 118, Loss: 0.2473, LR: 5.00e-04\n",
            "Epoch 119, Loss: 0.2425, LR: 5.00e-04\n",
            "Epoch 120, Loss: 0.2393, LR: 5.00e-04\n",
            "Epoch 121, Loss: 0.2422, LR: 5.00e-04\n",
            "Epoch 122, Loss: 0.2431, LR: 5.00e-04\n",
            "Epoch 123, Loss: 0.2432, LR: 5.00e-04\n",
            "Epoch 124, Loss: 0.2371, LR: 5.00e-04\n",
            "Epoch 125, Loss: 0.2374, LR: 5.00e-04\n",
            "Epoch 126, Loss: 0.2380, LR: 5.00e-04\n",
            "Epoch 127, Loss: 0.2419, LR: 5.00e-04\n",
            "Epoch 128, Loss: 0.2386, LR: 5.00e-04\n",
            "Epoch 129, Loss: 0.2353, LR: 5.00e-04\n",
            "Epoch 130, Loss: 0.2392, LR: 5.00e-04\n",
            "Epoch 131, Loss: 0.2354, LR: 5.00e-04\n",
            "Epoch 132, Loss: 0.2349, LR: 5.00e-04\n",
            "Epoch 133, Loss: 0.2354, LR: 5.00e-04\n",
            "Epoch 134, Loss: 0.2377, LR: 5.00e-04\n",
            "Epoch 135, Loss: 0.2351, LR: 5.00e-04\n",
            "Epoch 136, Loss: 0.2364, LR: 5.00e-04\n",
            "Epoch 137, Loss: 0.2305, LR: 5.00e-04\n",
            "Epoch 138, Loss: 0.2377, LR: 5.00e-04\n",
            "Epoch 139, Loss: 0.2333, LR: 5.00e-04\n",
            "Epoch 140, Loss: 0.2342, LR: 5.00e-04\n",
            "Epoch 141, Loss: 0.2340, LR: 5.00e-04\n",
            "Epoch 142, Loss: 0.2332, LR: 5.00e-04\n",
            "Epoch 143, Loss: 0.2336, LR: 2.50e-04\n",
            "Epoch 144, Loss: 0.2221, LR: 2.50e-04\n",
            "Epoch 145, Loss: 0.2214, LR: 2.50e-04\n",
            "Epoch 146, Loss: 0.2208, LR: 2.50e-04\n",
            "Epoch 147, Loss: 0.2189, LR: 2.50e-04\n",
            "Epoch 148, Loss: 0.2172, LR: 2.50e-04\n",
            "Epoch 149, Loss: 0.2197, LR: 2.50e-04\n",
            "Epoch 150, Loss: 0.2165, LR: 2.50e-04\n"
          ]
        }
      ],
      "source": [
        "best_loss = float('inf')\n",
        "patience = 10\n",
        "trigger_times = 0\n",
        "\n",
        "for epoch in range(150):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for _ in range(100):\n",
        "        batch = next(generate_chunk())\n",
        "        inputs = torch.tensor(batch[:, :-1], dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(batch[:, 1:], dtype=torch.long).to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        logits, _ = model(inputs)\n",
        "        loss = criterion(logits.permute(0,2,1), targets)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 5.0)  # Gradient clipping\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    avg_loss = total_loss / 100\n",
        "    scheduler.step(avg_loss)\n",
        "    \n",
        "    # Early stopping\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        trigger_times = 0\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "    else:\n",
        "        trigger_times += 1\n",
        "        if trigger_times >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.2e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFhUEmkYAMb6"
      },
      "source": [
        "В качестве иллюстрации ниже доступен график значений функции потерь, построенный в ходе обучения авторской сети (сам код для ее обучения вам и предстоит написать)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShVEcyPEAMb6"
      },
      "source": [
        "Шаблон функции `generate_sample` также доступен ниже. Вы можете как дозаполнить его, так и написать свою собственную функцию с нуля. Не забывайте, что все примеры в обучающей выборке начинались с токена `<sos>`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsNpZcXRAMb7"
      },
      "outputs": [],
      "source": [
        "def generate_sample(\n",
        "    char_rnn, seed_phrase=None, max_length=200, temperature=1.0, device=device\n",
        "):\n",
        "    \"\"\"\n",
        "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
        "    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n",
        "    :param max_length: maximum output length, including seed_phrase\n",
        "    :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs,\n",
        "                        smaller temperature converges to the single most likely output\n",
        "    \"\"\"\n",
        "\n",
        "    if seed_phrase is not None:\n",
        "        x_sequence = [token_to_idx[\"<sos>\"]] + [\n",
        "            token_to_idx[token] for token in seed_phrase\n",
        "        ]\n",
        "    else:\n",
        "        x_sequence = [token_to_idx[\"<sos>\"]]\n",
        "\n",
        "    x_sequence = torch.tensor([x_sequence], dtype=torch.int64).to(device)\n",
        "\n",
        "    # feed the seed phrase, if any\n",
        "\n",
        "    # your code here\n",
        "\n",
        "    return \"\".join([tokens[ix] for ix in x_sequence.cpu().data.numpy()[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_sample(\n",
        "    model, seed_phrase, max_length=500, temperature=0.7, top_k=20\n",
        "):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x_sequence = [token_to_idx[\"<sos>\"]] + [token_to_idx[c] for c in seed_phrase]\n",
        "        input_tensor = torch.tensor([x_sequence], dtype=torch.long).to(device)\n",
        "        hidden = None\n",
        "        \n",
        "        for _ in range(max_length - len(x_sequence)):\n",
        "            logits, hidden = model(input_tensor[:, -1:], hidden)\n",
        "            logits = logits[:, -1] / temperature\n",
        "            \n",
        "            # Top-K filtering\n",
        "            top_k_logits, top_k_indices = logits.topk(top_k, dim=-1)\n",
        "            probs = F.softmax(top_k_logits, dim=-1)\n",
        "            \n",
        "            next_idx = torch.multinomial(probs, 1)\n",
        "            next_idx = top_k_indices.gather(-1, next_idx)\n",
        "            \n",
        "            input_tensor = torch.cat([input_tensor, next_idx], dim=1)\n",
        "            \n",
        "        result = input_tensor.cpu().numpy()[0]\n",
        "        return ''.join([idx_to_token[idx] for idx in result]).replace('<sos>', '')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejyfZH6CAMb7"
      },
      "source": [
        "Пример текста сгенерированного обученной моделью доступен ниже. Не страшно, что в тексте много несуществующих слов. Используемая модель очень проста: это простая классическая RNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "xgiaZm4YAMb7",
        "outputId": "214b1319-e086-45c4-fbf1-aa22159318c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " мой дядя самых честных правильенные страны,\n",
            "где долго в лоно тишины\n",
            "лились его живые слезы;\n",
            "он пел поблеклый жизни цвет\n",
            "без малого в осьмнадцать лет.\n",
            "\n",
            "\n",
            "\n",
            "xi\n",
            "\n",
            "в пустыне, где один евгений\n",
            "мог оценить его дары,\n",
            "господ соседственных селений\n",
            "ему не нравились пиры;\n",
            "бежал он их беседы шумной,\n",
            "их разговор благоразумный\n",
            "о сенокосе, о вине,\n",
            "о псарне, о своей родне,\n",
            "конечно, не блистал ни чувством,\n",
            "ни поэтическим огнем,\n",
            "ни остротою, ни умом,\n",
            "ни общежития искусством;\n",
            "но разговор их милых жен\n",
            "гораздо меньше\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    generate_sample(\n",
        "        model, \" мой дядя самых честных правил\", max_length=500, temperature=0.8\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSbXHbFVAMb7"
      },
      "source": [
        "### Сдача задания\n",
        "Сгенерируйте десять последовательностей длиной 500, используя строку ' мой дядя самых честных правил'. Температуру для генерации выберите самостоятельно на основании визуального качества генериуремого текста. Не забудьте удалить все технические токены в случае их наличия.\n",
        "\n",
        "Сгенерированную последовательность сохрание в переменную `generated_phrase` и сдайте сгенерированный ниже файл в контест."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "aE86QOV-AMb7"
      },
      "outputs": [],
      "source": [
        "seed_phrase = \" мой дядя самых честных правил\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generated_phrases = [\n",
        "    generate_sample(\n",
        "        model,\n",
        "        ' мой дядя самых честных правил',\n",
        "        max_length=501,\n",
        "        temperature=0.7\n",
        "    ).replace('<sos>', '')  # Удаляем служебный токен\n",
        "    for _ in range(10)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "generated_phrases = [\n",
        "    generate_sample(model, seed_phrase, temperature=0.7, top_k=15)\n",
        "    for _ in range(10)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generated_phrases.__len__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "499\n",
            "499\n",
            "499\n",
            "499\n",
            "499\n",
            "499\n",
            "499\n",
            "499\n",
            "499\n",
            "499\n"
          ]
        }
      ],
      "source": [
        "for phrase in generated_phrases:\n",
        "    print(len(phrase))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "17hwljJkAMb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File saved to `submission_dict_hw09.npy`\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "\n",
        "if \"generated_phrases\" not in locals():\n",
        "    raise ValueError(\"Please, save generated phrases to `generated_phrases` variable\")\n",
        "\n",
        "for phrase in generated_phrases:\n",
        "\n",
        "    if not isinstance(phrase, str):\n",
        "        raise ValueError(\"The generated phrase should be a string\")\n",
        "\n",
        "    if len(phrase) != 499:\n",
        "        raise ValueError(\"The `generated_phrase` length should be equal to 500\")\n",
        "\n",
        "    assert all(\n",
        "        [x in set(tokens) for x in set(list(phrase))]\n",
        "    ), \"Unknown tokens detected, check your submission!\"\n",
        "\n",
        "\n",
        "submission_dict = {\"token_to_idx\": token_to_idx, \"generated_phrases\": generated_phrases}\n",
        "\n",
        "np.save(\"submission_dict_hw09.npy\", submission_dict, allow_pickle=True)\n",
        "print(\"File saved to `submission_dict_hw09.npy`\")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lkbux2JrAMb7"
      },
      "source": [
        "На этом задание завершено. Поздравляем!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "NLP HW Lab01_Poetry_generation.v5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
